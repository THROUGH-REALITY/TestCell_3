{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = .1    # 探索率\n",
    "ALPHA = .1      # 学習率\n",
    "GAMMA = .90     # 割引率\n",
    "ACTIONS = np.arange(4)  # 行動の集合\n",
    "\n",
    "class Summon:\n",
    "\n",
    "    def __init__(self, zero_list, population=2):\n",
    "        self.agents = self.__generate_agents(zero_list, population)\n",
    "\n",
    "    def __generate_agents(self, zero_list, population):\n",
    "        agents = []\n",
    "        for id in range(population):\n",
    "            ini_state = random.choice(zero_list) # 初期状態（エージェントのスタート地点の位置）\n",
    "            agents.append(\n",
    "                QLearningAgent(\n",
    "                    alpha=ALPHA,\n",
    "                    gamma=GAMMA,\n",
    "                    epsilon=EPSILON,\n",
    "                    actions=ACTIONS,\n",
    "                    observation=ini_state)) \n",
    "        return agents\n",
    "\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "        Q学習 エージェント\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=.2,\n",
    "            epsilon=.1,\n",
    "            gamma=.99,\n",
    "            actions=None,\n",
    "            observation=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_history = []\n",
    "        self.actions = actions\n",
    "        self.observation = observation\n",
    "        self.state = str(observation)\n",
    "        self.ini_state = str(observation)\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.q_values = self._init_q_values()\n",
    "\n",
    "    def _init_q_values(self):\n",
    "        \"\"\"\n",
    "           Q テーブルの初期化\n",
    "        \"\"\"\n",
    "        q_values = {}\n",
    "        q_values[self.state] = np.repeat(0.0, len(self.actions))\n",
    "        return q_values\n",
    "\n",
    "    def init_state(self):\n",
    "        \"\"\"\n",
    "            状態の初期化\n",
    "        \"\"\"\n",
    "        self.previous_state = copy.deepcopy(self.ini_state)\n",
    "        self.state = copy.deepcopy(self.ini_state)\n",
    "        return self.state\n",
    "\n",
    "    def act(self):\n",
    "        # ε-greedy選択\n",
    "        if np.random.uniform() < self.epsilon:  # random 行動\n",
    "            action = np.random.randint(0, len(self.q_values[self.state]))\n",
    "        else:   # greedy 行動\n",
    "            action = np.argmax(self.q_values[self.state])\n",
    "\n",
    "        self.previous_action = action\n",
    "        return action\n",
    "\n",
    "    def observe(self, next_state, reward=None):\n",
    "        \"\"\"\n",
    "            次の状態と報酬の観測\n",
    "        \"\"\"\n",
    "        next_state = str(next_state)\n",
    "        if next_state not in self.q_values:  # 始めて訪れる状態であれば\n",
    "            self.q_values[next_state] = np.repeat(0.0, len(self.actions))\n",
    "\n",
    "        self.previous_state = copy.deepcopy(self.state)\n",
    "        self.state = next_state\n",
    "\n",
    "        if reward is not None:\n",
    "            self.reward_history.append(reward)\n",
    "            self.learn(reward)\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "            Q値の更新\n",
    "        \"\"\"\n",
    "        q = self.q_values[self.previous_state][self.previous_action]  # Q(s, a)\n",
    "        max_q = max(self.q_values[self.state])  # max Q(s')\n",
    "        # Q(s, a) = Q(s, a) + alpha*(r+gamma*maxQ(s')-Q(s, a))\n",
    "        self.q_values[self.previous_state][self.previous_action] = q + \\\n",
    "            (self.alpha * (reward + (self.gamma * max_q) - q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    def __init__(self, x_max, y_max):\n",
    "\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.filed_type = {\n",
    "            \"N\": 0,  # 通常\n",
    "            \"G\": 1,  # ゴール\n",
    "            \"W\": 2,  # 壁\n",
    "            \"H\": 3,  # 他の人間\n",
    "        }\n",
    "        self.actions = {\n",
    "            \"UP\": 0,\n",
    "            \"DOWN\": 1,\n",
    "            \"LEFT\": 2,\n",
    "            \"RIGHT\": 3\n",
    "        }\n",
    "        self.map = np.zeros((y_max,x_max))\n",
    "        self.map[::2] = 2 \n",
    "        self.map[:, ::2] = 0\n",
    "        self.map[0,0] = 1\n",
    "\n",
    "        self.zero_list = list(zip(*np.where( self.map < 1)))\n",
    "\n",
    "        #self.start_pos = start_x,start_y   # エージェントのスタート地点(x, y)\n",
    "        #self.agent_pos = copy.deepcopy(self.start_pos)  # エージェントがいる地点\n",
    "\n",
    "    def step(self, start_x, start_y, action):\n",
    "        \"\"\"\n",
    "            行動の実行\n",
    "            状態, 報酬、ゴールしたかを返却\n",
    "        \"\"\"\n",
    "        to_x, to_y = start_x, start_y\n",
    "\n",
    "        # 移動可能かどうかの確認。移動不可能であれば、ポジションはそのままにマイナス報酬\n",
    "        if self._is_possible_action(to_x, to_y, action) == False:\n",
    "            self.agent_pos = to_x, to_y\n",
    "            return self.agent_pos, -10, False\n",
    "\n",
    "        if action == self.actions[\"UP\"]:\n",
    "            to_y += -1\n",
    "        elif action == self.actions[\"DOWN\"]:\n",
    "            to_y += 1\n",
    "        elif action == self.actions[\"LEFT\"]:\n",
    "            to_x += -1\n",
    "        elif action == self.actions[\"RIGHT\"]:\n",
    "            to_x += 1\n",
    "\n",
    "        is_goal = self._is_end_episode(to_x, to_y)  # エピソードの終了の確認\n",
    "        reward = self._compute_reward(to_x, to_y)\n",
    "        self.agent_pos = to_x, to_y\n",
    "        return self.agent_pos, reward, is_goal\n",
    "\n",
    "    def _is_end_episode(self, x, y):\n",
    "        \"\"\"\n",
    "            x, yがエピソードの終了かの確認。\n",
    "        \"\"\"\n",
    "        if self.map[y,x] == self.filed_type[\"G\"]: # ゴール\n",
    "            return True\n",
    "        #elif self.map[y][x] == self.filed_type[\"T\"]:    # トラップ\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _is_wall(self, x, y):\n",
    "        \"\"\"\n",
    "            x, yが壁または人間かどうかの確認\n",
    "        \"\"\"\n",
    "        if self.map[y,x] == self.filed_type[\"W\"]:\n",
    "            return True\n",
    "        elif self.map[y,x] == self.filed_type[\"H\"]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _is_possible_action(self, x, y, action):\n",
    "        \"\"\"\n",
    "            実行可能な行動かどうかの判定\n",
    "        \"\"\"\n",
    "        to_x = x\n",
    "        to_y = y\n",
    "\n",
    "        if action == self.actions[\"UP\"]:\n",
    "            print(\"下に行った\")\n",
    "            to_y += -1\n",
    "            #print(to_y,to_x)\n",
    "        elif action == self.actions[\"DOWN\"]:\n",
    "            print(\"上に行った\")\n",
    "            to_y += 1\n",
    "            #print(to_y,to_x)\n",
    "        elif action == self.actions[\"LEFT\"]:\n",
    "            print(\"左に行った\")\n",
    "            to_x += -1\n",
    "            #print(to_y,to_x)\n",
    "        elif action == self.actions[\"RIGHT\"]:\n",
    "            print(\"右に行った\")\n",
    "            to_x += 1\n",
    "            #print(to_y,to_x)\n",
    "\n",
    "        if self.map.shape[0] <= to_y or 0 > to_y:\n",
    "            print(\"y行き過ぎ\")\n",
    "            return False\n",
    "        elif self.map.shape[1] <= to_x or 0 > to_x:\n",
    "            print(\"x行き過ぎ\")\n",
    "            return False\n",
    "        elif self._is_wall(to_x, to_y):\n",
    "            print(\"壁だった\")\n",
    "            #print(to_y,to_x)\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _compute_reward(self, x, y):\n",
    "        if self.map[y,x] == self.filed_type[\"N\"]:\n",
    "            return 0\n",
    "        elif self.map[y,x] == self.filed_type[\"G\"]:\n",
    "            return 100\n",
    "        #elif self.map[y,x] == self.filed_type[\"T\"]:\n",
    "            return -100\n",
    "\n",
    "    def reset(self):\n",
    "        self.map = np.zeros((self.y_max,self.x_max))\n",
    "        self.map[::2] = 2 \n",
    "        self.map[:, ::2] = 0\n",
    "        self.map[0,0] = 1\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self.start_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数\n",
    "NB_EPISODE = 1   # エピソード数\n",
    "X_MAX = 15\n",
    "Y_MAX = 18\n",
    "POPULATION = 2\n",
    "start = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_env = GridWorld(   # grid worldの環境の初期化\n",
    "        x_max=X_MAX,\n",
    "        y_max=Y_MAX,)\n",
    "\n",
    "    summon = Summon(    # エージェントの召喚\n",
    "        zero_list=grid_env.zero_list,\n",
    "        population=POPULATION)\n",
    "    \n",
    "    times = []\n",
    "    is_end_episode = []\n",
    "    for episode in range(NB_EPISODE):   # 実験\n",
    "        #episode_reward = []  # 1エピソードの累積報酬\n",
    "        for i in range(POPULATION):\n",
    "            start_x = summon.agents[i].observation[0]\n",
    "            start_y = summon.agents[i].observation[1]\n",
    "            grid_env.map[start_x][start_y] = 3\n",
    "            is_end_episode.append(False)\n",
    "            print(start_x,start_y)\n",
    "        plt.imshow(grid_env.map)\n",
    "        plt.show()\n",
    "        print(is_end_episode)\n",
    "        while(False in is_end_episode):    # 全員がゴールするまで続ける\n",
    "            for id in range(POPULATION):\n",
    "                if is_end_episode[id] == False:\n",
    "                    start_x = summon.agents[id].observation[0]\n",
    "                    start_y = summon.agents[id].observation[1]\n",
    "                    grid_env.map[start_x][start_y] = 0\n",
    "                    print(start_x,start_y)\n",
    "                    action = summon.agents[id].act()  # 行動選択\n",
    "                    print(action)\n",
    "                    state, reward, is_end_episode[id] = grid_env.step(start_x, start_y, action)\n",
    "                else: grid_env.map[0,0] = 1\n",
    "                if is_end_episode[id] == False:\n",
    "                    grid_env.map[state[0]][state[1]] = 3\n",
    "                    print(state,reward)\n",
    "                    summon.agents[id].observe(state, reward)   # 状態と報酬の観測\n",
    "                plt.imshow(grid_env.map)\n",
    "                plt.show()\n",
    "        print(is_end_episode)    \n",
    "            #episode_reward.append(reward)\n",
    "            #rewards.append(np.sum(episode_reward))  # このエピソードの平均報酬を与える\n",
    "        #times.append(len(episode_reward)) #かかった時間をリストに追加\n",
    "        state = grid_env.reset()  # 初期化\n",
    "        #agent.observe(state)    # エージェントを初期位置に\n",
    "        print(f\"EP.{episode +1} End\") #(t = {len(episode_reward)})\"\n",
    "\n",
    "    # 所要時間の計算\n",
    "    print(f\"time = {time.time()-start}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
